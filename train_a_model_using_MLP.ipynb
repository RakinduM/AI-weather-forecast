{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnqp4YGcHgBhJqNV7l29+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RakinduM/AI-weather-forecast/blob/feat%2Ftrain-a-model-using-MLP/train_a_model_using_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup — packages and runtime notes"
      ],
      "metadata": {
        "id": "MX20hvElw8GT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYO7jbecwt1e"
      },
      "outputs": [],
      "source": [
        "# Colab-specific: install packages if necessary (uncomment if you need to install)\n",
        "# !pip install -q tensorflow==2.11.0 scikit-learn joblib\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report, roc_curve, auc\n",
        "from sklearn.utils import class_weight\n",
        "import joblib\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "\n",
        "# Path to CSV (change if needed)\n",
        "DATA_PATH = \"/mnt/data/weatherAUS 2.csv\"  # <-- update if your file path differs\n",
        "\n",
        "# Check GPU (Colab: Runtime -> Change runtime type -> GPU)\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load dataset"
      ],
      "metadata": {
        "id": "765UDYkQxDcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "assert os.path.exists(DATA_PATH), f\"CSV not found at {DATA_PATH}. Upload to that path or change DATA_PATH.\"\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "print(\"Dataset loaded. Shape:\", df.shape)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "KsOQqaPxxGbd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quick EDA (do this to understand the dataset)"
      ],
      "metadata": {
        "id": "7OH02HjBxKxD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic info\n",
        "print(df.info())\n",
        "print(\"\\nTarget counts (include NaNs):\")\n",
        "if 'RainTomorrow' in df.columns:\n",
        "    print(df['RainTomorrow'].value_counts(dropna=False))\n",
        "else:\n",
        "    raise ValueError(\"RainTomorrow target column not found in dataset.\")\n",
        "\n",
        "# Percent missing per column\n",
        "missing = df.isnull().mean().sort_values(ascending=False)\n",
        "print(\"\\nMissing values fraction:\")\n",
        "print(missing[missing > 0].head(30))\n",
        "\n",
        "# Plot target balance\n",
        "plt.figure(figsize=(5,4))\n",
        "df['RainTomorrow'].map({'Yes':1,'No':0}).value_counts().plot(kind='bar')\n",
        "plt.title('RainTomorrow distribution (raw)')\n",
        "plt.xticks([0,1], ['No','Yes'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wMLOVP8IxPaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing & Feature Engineering\n",
        "\n",
        "Notes:\n",
        "\n",
        "We'll produce a clean, reproducible preprocessing pipeline in a Preprocessor class to use when training and later for inference.\n",
        "\n",
        "Strategy:\n",
        "\n",
        "- Convert date → Month, DayOfWeek, Season\n",
        "\n",
        "- Numeric: impute median, then StandardScaler\n",
        "\n",
        "- Categorical: fill missing with 'Missing', then One-Hot (for MLP)\n",
        "\n",
        "- Optionally create lag features (yesterday rain) — kept commented but included"
      ],
      "metadata": {
        "id": "zHu6WGnBxTCf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing configuration\n",
        "NUMERIC_FEATURES = [\n",
        "    'MinTemp','MaxTemp','Rainfall','Evaporation','Sunshine',\n",
        "    'WindGustSpeed','WindSpeed9am','WindSpeed3pm',\n",
        "    'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm',\n",
        "    'Temp9am','Temp3pm'\n",
        "]\n",
        "# intersection with actual columns\n",
        "NUMERIC_FEATURES = [c for c in NUMERIC_FEATURES if c in df.columns]\n",
        "\n",
        "CATEGORICAL_FEATURES = ['Location','WindGustDir','WindDir9am','WindDir3pm']\n",
        "CATEGORICAL_FEATURES = [c for c in CATEGORICAL_FEATURES if c in df.columns]\n",
        "\n",
        "TARGET = 'RainTomorrow'\n",
        "\n",
        "print(\"Numeric features used:\", NUMERIC_FEATURES)\n",
        "print(\"Categorical features used:\", CATEGORICAL_FEATURES)\n"
      ],
      "metadata": {
        "id": "kvt5FQy-xhuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessor class (fit on training data, transform train/test/inference)"
      ],
      "metadata": {
        "id": "Qb_a70dYxnQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PreprocessorMLP:\n",
        "    def __init__(self, numeric_features, categorical_features, scaler=None, ohe=None):\n",
        "        self.numeric_features = numeric_features\n",
        "        self.categorical_features = categorical_features\n",
        "        self.scaler = scaler if scaler is not None else StandardScaler()\n",
        "        self.num_imputer = SimpleImputer(strategy='median')\n",
        "        self.cat_imputer = SimpleImputer(strategy='constant', fill_value='Missing')\n",
        "        self.ohe = ohe if ohe is not None else OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
        "        self.fitted = False\n",
        "\n",
        "    def fit(self, df):\n",
        "        # Dates -> features\n",
        "        if 'Date' in df.columns:\n",
        "            df = df.copy()\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "            # optional season\n",
        "            df['Season'] = ((df['Month'] % 12 + 3) // 3)\n",
        "            # keep season as category\n",
        "            if 'Season' not in self.categorical_features:\n",
        "                self.categorical_features = self.categorical_features + ['Season']\n",
        "        # Numeric pipeline fit\n",
        "        X_num = df[self.numeric_features]\n",
        "        X_num = self.num_imputer.fit_transform(X_num)\n",
        "        self.scaler.fit(X_num)\n",
        "        # Categorical fit\n",
        "        X_cat = df[self.categorical_features].astype(str)\n",
        "        X_cat = self.cat_imputer.fit_transform(X_cat)\n",
        "        self.ohe.fit(X_cat)\n",
        "        self.fitted = True\n",
        "\n",
        "    def transform(self, df):\n",
        "        assert self.fitted, \"Call fit first.\"\n",
        "        df = df.copy()\n",
        "        if 'Date' in df.columns:\n",
        "            df['Date'] = pd.to_datetime(df['Date'])\n",
        "            df['Month'] = df['Date'].dt.month\n",
        "            df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "            df['Season'] = ((df['Month'] % 12 + 3)//3)\n",
        "            if 'Season' not in self.categorical_features:\n",
        "                self.categorical_features = self.categorical_features + ['Season']\n",
        "        # numeric transform\n",
        "        X_num = df[self.numeric_features]\n",
        "        X_num = self.num_imputer.transform(X_num)\n",
        "        X_num = self.scaler.transform(X_num)\n",
        "        # categorical transform\n",
        "        X_cat = df[self.categorical_features].astype(str)\n",
        "        X_cat = self.cat_imputer.transform(X_cat)\n",
        "        X_ohe = self.ohe.transform(X_cat)\n",
        "        # final concatenated input for MLP\n",
        "        X = np.hstack([X_num, X_ohe])\n",
        "        return X\n",
        "\n",
        "    def save(self, path):\n",
        "        joblib.dump({\n",
        "            'num_imputer': self.num_imputer,\n",
        "            'scaler': self.scaler,\n",
        "            'cat_imputer': self.cat_imputer,\n",
        "            'ohe': self.ohe,\n",
        "            'numeric_features': self.numeric_features,\n",
        "            'categorical_features': self.categorical_features\n",
        "        }, path)\n",
        "\n",
        "    def load(self, path):\n",
        "        data = joblib.load(path)\n",
        "        self.num_imputer = data['num_imputer']\n",
        "        self.scaler = data['scaler']\n",
        "        self.cat_imputer = data['cat_imputer']\n",
        "        self.ohe = data['ohe']\n",
        "        self.numeric_features = data['numeric_features']\n",
        "        self.categorical_features = data['categorical_features']\n",
        "        self.fitted = True\n"
      ],
      "metadata": {
        "id": "CshPV_ECxsSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train/Test split + fit preprocessor"
      ],
      "metadata": {
        "id": "jhKNo6I-xvHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows with missing target\n",
        "df = df[~df[TARGET].isna()].copy()\n",
        "df[TARGET] = df[TARGET].map({'Yes':1,'No':0})\n",
        "\n",
        "# Optionally reduce dataset size in early experiments:\n",
        "# df = df.sample(frac=0.5, random_state=RANDOM_SEED)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df[TARGET], random_state=RANDOM_SEED)\n",
        "\n",
        "pre = PreprocessorMLP(NUMERIC_FEATURES, CATEGORICAL_FEATURES)\n",
        "pre.fit(train_df)\n",
        "\n",
        "X_train = pre.transform(train_df)\n",
        "X_test  = pre.transform(test_df)\n",
        "\n",
        "y_train = train_df[TARGET].values\n",
        "y_test  = test_df[TARGET].values\n",
        "\n",
        "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n"
      ],
      "metadata": {
        "id": "M66BgXp5xyZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the MLP model (Keras)\n",
        "\n",
        "Design notes:\n",
        "\n",
        "- Input dimension = number of numerical features + one-hot categorical dims\n",
        "\n",
        "- Simple but effective architecture with dropout & batchnorm\n",
        "\n",
        "- Output: sigmoid for binary classification"
      ],
      "metadata": {
        "id": "PPj275NNx1EC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mlp(input_dim, hidden_units=[256,128,64], dropout_rate=0.3, l2_reg=1e-4):\n",
        "    inputs = keras.Input(shape=(input_dim,), name='input')\n",
        "    x = inputs\n",
        "    for i, u in enumerate(hidden_units):\n",
        "        x = layers.Dense(u, activation=None, kernel_regularizer=keras.regularizers.l2(l2_reg))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = layers.Activation('relu')(x)\n",
        "        x = layers.Dropout(dropout_rate)(x)\n",
        "    outputs = layers.Dense(1, activation='sigmoid', name='output')(x)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs, name='MLP')\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=[keras.metrics.BinaryAccuracy(name='accuracy'), keras.metrics.AUC(name='auc')])\n",
        "    return model\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "model = create_mlp(input_dim=input_dim, hidden_units=[256,128,64], dropout_rate=0.3)\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "45Du1xXWx64H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training: callbacks, class weights, and fit"
      ],
      "metadata": {
        "id": "8iaSmkJfx-Uv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights to handle imbalance\n",
        "class_weights_vals = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weights = {0: class_weights_vals[0], 1: class_weights_vals[1]}\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# Callbacks\n",
        "earlystop = keras.callbacks.EarlyStopping(monitor='val_auc', mode='max', patience=6, restore_best_weights=True)\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('mlp_best.h5', monitor='val_auc', mode='max', save_best_only=True, verbose=1)\n",
        "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_auc', mode='max', factor=0.5, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "history = model.fit(X_train, y_train,\n",
        "                    validation_split=0.1,\n",
        "                    epochs=60,\n",
        "                    batch_size=512,\n",
        "                    class_weight=class_weights,\n",
        "                    callbacks=[earlystop, checkpoint, reduce_lr])\n"
      ],
      "metadata": {
        "id": "3LRFRaehyDJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation & visualization"
      ],
      "metadata": {
        "id": "gDWZIgNtyGGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load best weights (ModelCheckpoint saved best model)\n",
        "model.load_weights('mlp_best.h5')\n",
        "\n",
        "# Predictions\n",
        "y_proba = model.predict(X_test).ravel()\n",
        "y_pred = (y_proba >= 0.5).astype(int)\n",
        "\n",
        "# Metrics\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "rec = recall_score(y_test, y_pred, zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "try:\n",
        "    roc = roc_auc_score(y_test, y_proba)\n",
        "except Exception as e:\n",
        "    roc = None\n",
        "\n",
        "print(\"Test results:\")\n",
        "print(f\"Accuracy: {acc:.4f}  Precision: {prec:.4f}  Recall: {rec:.4f}  F1: {f1:.4f}  AUC: {roc:.4f}\")\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(5,4))\n",
        "plt.imshow(cm, interpolation='nearest')\n",
        "plt.title(\"Confusion matrix\")\n",
        "plt.colorbar()\n",
        "plt.xticks([0,1], ['No','Yes'])\n",
        "plt.yticks([0,1], ['No','Yes'])\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        plt.text(j, i, cm[i,j], ha='center', va='center', color='white' if cm[i,j]>cm.max()/2 else 'black')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.show()\n",
        "\n",
        "# ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(fpr, tpr, label=f\"AUC = {roc:.4f}\")\n",
        "plt.plot([0,1],[0,1],'--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Training curves\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.title('Loss')\n",
        "plt.legend()\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(history.history['auc'], label='train_auc')\n",
        "plt.plot(history.history['val_auc'], label='val_auc')\n",
        "plt.title('AUC')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "N56NQlAIyIvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save artifacts (preprocessor + model + metadata)"
      ],
      "metadata": {
        "id": "39pUMeQhyLxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save preprocessor\n",
        "pre.save('preprocessor_mlp.joblib')\n",
        "\n",
        "# Save final model (Keras .h5 already saved by checkpoint)\n",
        "model.save('mlp_final_model.h5')\n",
        "\n",
        "# Save a simple metadata JSON\n",
        "import json\n",
        "meta = {\n",
        "    'numeric_features': pre.numeric_features,\n",
        "    'categorical_features': pre.categorical_features,\n",
        "    'target': TARGET,\n",
        "    'model_file': 'mlp_final_model.h5',\n",
        "    'preprocessor_file': 'preprocessor_mlp.joblib'\n",
        "}\n",
        "with open('mlp_metadata.json', 'w') as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "print(\"Saved model and preprocessor to disk.\")\n"
      ],
      "metadata": {
        "id": "pJG3MzIfyPK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "7oggvnPbySbd"
      }
    }
  ]
}